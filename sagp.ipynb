{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b12d801",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', str(text).strip()) if s]\n",
    "\n",
    "def load_and_process_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    processed = []\n",
    "    for _, row in df.iterrows():\n",
    "        claim = row['Statement']\n",
    "        context = row['Context']\n",
    "        evidence = row['Evidence']\n",
    "        label = int(row['labels'])\n",
    "\n",
    "        sentences = sentence_tokenize(context)\n",
    "        rationale_indices = [\n",
    "            i for i, s in enumerate(sentences)\n",
    "            if evidence.strip() in s or s in evidence.strip()\n",
    "        ]\n",
    "\n",
    "        processed.append({\n",
    "            \"claim\": claim,\n",
    "            \"evidences\": sentences,\n",
    "            \"label\": label,\n",
    "            \"rationale_indices\": rationale_indices\n",
    "        })\n",
    "    return processed\n",
    "\n",
    "train_data = load_and_process_csv(\"./data/train_clean.csv\")\n",
    "test_data = load_and_process_csv(\"./data/test_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7551f276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Train label distribution:\n",
      "  SUPPORTED (0): 1751 samples (51.36%)\n",
      "  REFUTED (1): 1658 samples (48.64%)\n",
      "\n",
      "📊 Test label distribution:\n",
      "  REFUTED (1): 468 samples (47.95%)\n",
      "  SUPPORTED (0): 508 samples (52.05%)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Đếm số lượng mỗi nhãn trong train/test\n",
    "train_labels = [item[\"label\"] for item in train_data]\n",
    "test_labels = [item[\"label\"] for item in test_data]\n",
    "\n",
    "train_counts = Counter(train_labels)\n",
    "test_counts = Counter(test_labels)\n",
    "\n",
    "# Tính tỉ lệ %\n",
    "def display_label_stats(counts, name):\n",
    "    total = sum(counts.values())\n",
    "    print(f\"\\n📊 {name} label distribution:\")\n",
    "    for label, count in counts.items():\n",
    "        percent = 100 * count / total\n",
    "        name_label = \"SUPPORTED\" if label == 0 else \"REFUTED\"\n",
    "        print(f\"  {name_label} ({label}): {count} samples ({percent:.2f}%)\")\n",
    "\n",
    "# Hiển thị\n",
    "display_label_stats(train_counts, \"Train\")\n",
    "display_label_stats(test_counts, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d418572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def encode_graph_sample(sample, max_len=128):\n",
    "    inputs = [sample['claim'] + \" [SEP] \" + sent for sent in sample['evidences']]\n",
    "    encoding = tokenizer(inputs, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')\n",
    "\n",
    "    num_nodes = len(inputs)\n",
    "    if num_nodes < 2:\n",
    "        return None\n",
    "\n",
    "    edge_index = torch.combinations(torch.arange(num_nodes), r=2).T\n",
    "    edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1).long()\n",
    "\n",
    "    return Data(\n",
    "        input_ids=encoding['input_ids'],\n",
    "        attention_mask=encoding['attention_mask'],\n",
    "        edge_index=edge_index,\n",
    "        y=torch.tensor(sample['label']),\n",
    "        rationale=torch.tensor(sample['rationale_indices']),\n",
    "        num_nodes=num_nodes\n",
    "    )\n",
    "\n",
    "train_graph = [encode_graph_sample(s) for s in train_data if encode_graph_sample(s) is not None]\n",
    "test_graph = [encode_graph_sample(s) for s in test_data if encode_graph_sample(s) is not None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5737cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training graphs: 3409\n",
      "✅ Test graphs:     976\n"
     ]
    }
   ],
   "source": [
    "print(f\"✅ Training graphs: {len(train_graph)}\")\n",
    "print(f\"✅ Test graphs:     {len(test_graph)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def collate(batch):\n",
    "    return Batch.from_data_list(batch)\n",
    "\n",
    "train_loader = DataLoader(train_graph, batch_size=8, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(test_graph, batch_size=8, shuffle=False, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaModel\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class SaGP(nn.Module):\n",
    "    def __init__(self, hidden_dim=768):\n",
    "        super().__init__()\n",
    "        self.encoder = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.gcn1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, 2)\n",
    "        self.rationale_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, edge_index, batch_index):\n",
    "        with torch.no_grad():\n",
    "            out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        x = out.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        x = F.relu(self.gcn1(x, edge_index))\n",
    "        x = F.relu(self.gcn2(x, edge_index))\n",
    "\n",
    "        pooled = global_mean_pool(x, batch_index)  # [batch_size, hidden_dim]\n",
    "        logits = self.classifier(pooled)\n",
    "\n",
    "        rationale_scores = torch.sigmoid(self.rationale_head(x)).squeeze(-1)  # [num_nodes]\n",
    "        return logits, rationale_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9211404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 295.9503\n",
      "Epoch 2, Loss: 296.2538\n",
      "Epoch 3, Loss: 296.0361\n",
      "Epoch 4, Loss: 296.0292\n",
      "Epoch 5, Loss: 295.4590\n",
      "Epoch 6, Loss: 295.2324\n",
      "Epoch 7, Loss: 294.8454\n",
      "Epoch 8, Loss: 295.1835\n",
      "Epoch 9, Loss: 294.3911\n",
      "Epoch 10, Loss: 294.9427\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SaGP().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch.input_ids.to(device)\n",
    "        attention_mask = batch.attention_mask.to(device)\n",
    "        edge_index = batch.edge_index.to(device)\n",
    "        batch_index = batch.batch.to(device)\n",
    "        labels = batch.y.to(device)\n",
    "\n",
    "        logits, _ = model(input_ids, attention_mask, edge_index, batch_index)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e8924eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   SUPPORTED       0.52      0.97      0.68       508\n",
      "     REFUTED       0.60      0.04      0.08       468\n",
      "\n",
      "    accuracy                           0.53       976\n",
      "   macro avg       0.56      0.51      0.38       976\n",
      "weighted avg       0.56      0.53      0.39       976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch.input_ids.to(device)\n",
    "        attention_mask = batch.attention_mask.to(device)\n",
    "        edge_index = batch.edge_index.to(device)\n",
    "        batch_index = batch.batch.to(device)\n",
    "        labels = batch.y.to(device)\n",
    "\n",
    "        logits, _ = model(input_ids, attention_mask, edge_index, batch_index)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "print(\"Claim Classification:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"SUPPORTED\", \"REFUTED\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "011043eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated samples: 976\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluated samples: {len(all_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1d61acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "\n",
    "def evaluate_rationale(model, data_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_f1s = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch.input_ids.to(device)\n",
    "            attention_mask = batch.attention_mask.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            batch_index = batch.batch.to(device)\n",
    "\n",
    "            _, rationale_scores = model(input_ids, attention_mask, edge_index, batch_index)\n",
    "\n",
    "            graph_sizes = torch.bincount(batch_index).tolist()\n",
    "            num_graphs = len(graph_sizes)\n",
    "\n",
    "            # Tách lại từng Data sample từ batch\n",
    "            data_list = batch.to_data_list()\n",
    "\n",
    "            start = 0\n",
    "            for i, data_item in enumerate(data_list):\n",
    "                size = data_item.num_nodes\n",
    "                end = start + size\n",
    "                scores = rationale_scores[start:end].cpu()\n",
    "                pred_idxs = [j for j, s in enumerate(scores) if s > threshold]\n",
    "\n",
    "                # --- Lấy ground truth rationale ---\n",
    "                rationale_tensor = data_item.rationale\n",
    "                if isinstance(rationale_tensor, torch.Tensor):\n",
    "                    if rationale_tensor.ndim == 0:\n",
    "                        true_idxs = [int(rationale_tensor.item())]\n",
    "                    else:\n",
    "                        true_idxs = [int(x.item()) for x in rationale_tensor]\n",
    "                elif isinstance(rationale_tensor, (int, float)):\n",
    "                    true_idxs = [int(rationale_tensor)]\n",
    "                else:\n",
    "                    true_idxs = list(rationale_tensor)\n",
    "\n",
    "                true_idxs = [idx for idx in true_idxs if isinstance(idx, int) and idx >= 0]\n",
    "\n",
    "                # --- Tính Precision / Recall / F1 ---\n",
    "                if not true_idxs and not pred_idxs:\n",
    "                    precision = recall = f1 = 1.0\n",
    "                elif not pred_idxs:\n",
    "                    precision = recall = f1 = 0.0\n",
    "                else:\n",
    "                    true_set = set(true_idxs)\n",
    "                    pred_set = set(pred_idxs)\n",
    "                    tp = len(true_set & pred_set)\n",
    "                    precision = tp / len(pred_set) if pred_set else 0.0\n",
    "                    recall = tp / len(true_set) if true_set else 0.0\n",
    "                    f1 = 2 * precision * recall / (precision + recall + 1e-8) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "                all_precisions.append(precision)\n",
    "                all_recalls.append(recall)\n",
    "                all_f1s.append(f1)\n",
    "\n",
    "                start = end\n",
    "\n",
    "    print(\"Rationale Extraction Quality:\")\n",
    "    print(f\"Precision: {sum(all_precisions)/len(all_precisions):.3f}\")\n",
    "    print(f\"Recall:    {sum(all_recalls)/len(all_recalls):.3f}\")\n",
    "    print(f\"F1-score:  {sum(all_f1s)/len(all_f1s):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "893becc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rationale Extraction Quality:\n",
      "Precision: 0.053\n",
      "Recall:    0.684\n",
      "F1-score:  0.095\n"
     ]
    }
   ],
   "source": [
    "evaluate_rationale(model, test_loader, threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7c731bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rationale Extraction Quality:\n",
      "Precision: 0.266\n",
      "Recall:    0.388\n",
      "F1-score:  0.274\n"
     ]
    }
   ],
   "source": [
    "evaluate_rationale(model, test_loader, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45dfd8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rationale Extraction Quality:\n",
      "Precision: 0.316\n",
      "Recall:    0.316\n",
      "F1-score:  0.316\n"
     ]
    }
   ],
   "source": [
    "evaluate_rationale(model, test_loader, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59823f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"./model/sagp_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
