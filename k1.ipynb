{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c623b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Tách câu\n",
    "# -----------------------------\n",
    "def sentence_tokenize(text):\n",
    "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', str(text)) if s]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load dữ liệu\n",
    "# -----------------------------\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        claim = row[\"Statement\"]\n",
    "        context_sents = sentence_tokenize(row[\"Context\"])\n",
    "        label = row[\"labels\"]\n",
    "        evidence = row[\"Evidence\"]\n",
    "        data.append({\n",
    "            'claim': claim,\n",
    "            'sentences': context_sents,\n",
    "            'label': label,\n",
    "            'evidence': evidence\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Load test set\n",
    "test_data = load_data(\"./data/test_clean.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Load sentence-BERT\n",
    "# -----------------------------\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")  \n",
    "\n",
    "# -----------------------------\n",
    "# 4. Chọn Top-1 rationale\n",
    "# -----------------------------\n",
    "def extract_top1_rationale(claim, sentences):\n",
    "    emb_claim = sbert.encode(claim, convert_to_tensor=True)\n",
    "    emb_sents = sbert.encode(sentences, convert_to_tensor=True)\n",
    "    scores = util.cos_sim(emb_claim, emb_sents)[0]\n",
    "    best_idx = scores.argmax().item()\n",
    "    return sentences[best_idx], best_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756eeb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label_from_rationale(rationale_sent):\n",
    "    rationale_lower = rationale_sent.lower()\n",
    "    neg_words = [\"không\", \"chưa\", \"sai\", \"phủ nhận\", \"bác bỏ\"]\n",
    "    for word in neg_words:\n",
    "        if word in rationale_lower:\n",
    "            return 1  # REFUTED\n",
    "    return 0  # SUPPORTED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b8767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim Classification (Fully Unsupervised):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   SUPPORTED       0.53      0.84      0.65       508\n",
      "     REFUTED       0.50      0.17      0.26       468\n",
      "\n",
      "    accuracy                           0.52       976\n",
      "   macro avg       0.51      0.51      0.45       976\n",
      "weighted avg       0.51      0.52      0.46       976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = [], []\n",
    "\n",
    "for sample in test_data:\n",
    "    rationale_sent, _ = extract_top1_rationale(sample[\"claim\"], sample[\"sentences\"])\n",
    "    pred = predict_label_from_rationale(rationale_sent)\n",
    "    y_pred.append(pred)\n",
    "    y_true.append(sample[\"label\"])\n",
    "\n",
    "print(\"Claim Classification:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"SUPPORTED\", \"REFUTED\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3839f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rationale_quality(data):\n",
    "    matched, total = 0, 0\n",
    "    for sample in data:\n",
    "        rationale_sent, _ = extract_top1_rationale(sample[\"claim\"], sample[\"sentences\"])\n",
    "        evidence_text = str(sample[\"evidence\"]).strip()\n",
    "        if not evidence_text:\n",
    "            continue\n",
    "        total += 1\n",
    "        if evidence_text in rationale_sent or rationale_sent in evidence_text:\n",
    "            matched += 1\n",
    "    if total == 0:\n",
    "        print(\"Không có evidence hợp lệ.\")\n",
    "    else:\n",
    "        print(f\"Rationale Overlap: {matched}/{total} = {matched/total:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d211e9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rationale Overlap: 470/976 = 0.48\n"
     ]
    }
   ],
   "source": [
    "evaluate_rationale_quality(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d2aa876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_rationale_quality(data):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for sample in data:\n",
    "        context_sentences = sample[\"sentences\"]\n",
    "        evidence_text = str(sample.get(\"evidence\", \"\")).strip()\n",
    "        if not evidence_text:\n",
    "            continue\n",
    "\n",
    "        # Chuyển evidence gốc thành gold index (câu nào chứa đoạn evidence)\n",
    "        gold_labels = [1 if evidence_text in sent else 0 for sent in context_sentences]\n",
    "\n",
    "        # Dự đoán rationale bằng top-1 similarity (unsupervised)\n",
    "        rationale_sent, pred_idx = extract_top1_rationale(sample[\"claim\"], context_sentences)\n",
    "        pred_labels = [1 if i == pred_idx else 0 for i in range(len(context_sentences))]\n",
    "\n",
    "        # Gộp lại\n",
    "        all_preds.extend(pred_labels)\n",
    "        all_labels.extend(gold_labels)\n",
    "\n",
    "    if not all_preds:\n",
    "        print(\"No valid rationale to evaluate.\")\n",
    "        return\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(\"Rationale Extraction Quality (Unsupervised Top-1):\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall:    {recall:.3f}\")\n",
    "    print(f\"F1-score:  {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53d0ca29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rationale Extraction Quality (Unsupervised Top-1):\n",
      "Precision: 0.303\n",
      "Recall:    0.827\n",
      "F1-score:  0.444\n"
     ]
    }
   ],
   "source": [
    "evaluate_rationale_quality(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
